% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gdrm_fitters.R
\name{adam_control}
\alias{adam_control}
\title{Control Parameters for ADAM Optimization}
\usage{
adam_control(
  maxit = 1e+05,
  grad_tol = 1e-06,
  param_tol = 1e-10,
  require_both = FALSE,
  alpha = 0.1,
  beta1 = 0.9,
  beta2 = 0.999,
  epsilon = 1e-08,
  verbose = TRUE,
  expected = TRUE,
  lr_decay = TRUE,
  lr_decay_it = floor(0.5 * maxit),
  lr_decay_rate = 0.01
)
}
\arguments{
\item{maxit}{Integer. Maximum number of iterations. Default is 100000.}

\item{grad_tol}{Numeric. Gradient tolerance for convergence. Default is 1e-06.}

\item{param_tol}{Numeric. Parameter tolerance for convergence. Default is 1e-10.}

\item{require_both}{Logical. If TRUE, both gradient and parameter tolerances
must be satisfied for convergence. If FALSE, either condition is sufficient.
Default is FALSE.}

\item{alpha}{Numeric. Learning rate (step size) for ADAM optimization.
Default is 0.1.}

\item{beta1}{Numeric. Exponential decay rate for the first moment estimates.
Should be in [0, 1). Default is 0.9.}

\item{beta2}{Numeric. Exponential decay rate for the second moment estimates.
Should be in [0, 1). Default is 0.999.}

\item{epsilon}{Numeric. Small constant added to denominator for numerical
stability. Default is 1e-8.}

\item{verbose}{Logical. If TRUE, prints iteration progress. Default is TRUE.}

\item{expected}{Logical. Currently unused parameter for potential future
extensions. Default is TRUE.}

\item{lr_decay}{Logical. If TRUE, applies learning rate decay. Default is TRUE.}

\item{lr_decay_it}{Integer. Iteration interval for learning rate decay.
Default is floor(0.5 * maxit).}

\item{lr_decay_rate}{Numeric. Rate of learning rate decay. Default is 0.01.}
}
\value{
A named list containing all control parameters for ADAM optimization.
}
\description{
Control Parameters for ADAM Optimization
}
\details{
The learning rate decay is applied logarithmically when \code{lr_decay = TRUE}:
\deqn{\alpha_{new} = \frac{\alpha}{1 + \text{lr\_decay\_rate} \times \log(\text{it})}}
}
\seealso{
\code{\link{gdrm_adam}} for the main optimization function.
}
